from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    roc_curve,
    auc,
    precision_score,
    recall_score,
    f1_score,
)
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import joblib
import os


def train_model(df):
    # 1. Äáº·t nhÃ£n vÃ  Ä‘áº·c trÆ°ng
    X = df.drop("diabetic", axis=1)
    y = df["diabetic"]

    # 2. Chia dá»¯ liá»‡u train/test (stratify Ä‘áº£m báº£o tá»· lá»‡ lá»›p giá»¯ nguyÃªn á»Ÿ train/test)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # 3. Kiá»ƒm tra vÃ  bÃ¡o cÃ¡o phÃ¢n bá»• lá»›p
    print("\nðŸ“Š Tá»· lá»‡ toÃ n bá»™ dá»¯ liá»‡u:")
    print(y.value_counts())
    print(y.value_counts(normalize=True).apply(lambda x: f"{x:.2%}"))

    print("\nðŸ“Š Tá»· lá»‡ táº­p Train (trÆ°á»›c SMOTE):")
    print(y_train.value_counts())
    print(y_train.value_counts(normalize=True).apply(lambda x: f"{x:.2%}"))

    print("\nðŸ“Š Tá»· lá»‡ táº­p Test:")
    print(y_test.value_counts())
    print(y_test.value_counts(normalize=True).apply(lambda x: f"{x:.2%}"))

    # 4. SMOTE cÃ¢n báº±ng lá»›p trÃªn táº­p train (khÃ´ng bao giá» trÃªn test)
    print("\nðŸ” Ãp dá»¥ng SMOTE trÃªn táº­p Train...")
    smote = SMOTE(random_state=42)
    X_train_balanced, y_train_balanced = smote.fit_resample(
        X_train.astype("float64"), y_train.astype(int)
    )

    print("\nðŸ“Š Tá»· lá»‡ táº­p Train sau SMOTE (Ä‘Ã£ cÃ¢n báº±ng):")
    print(pd.Series(y_train_balanced).value_counts())
    print(
        pd.Series(y_train_balanced)
        .value_counts(normalize=True)
        .apply(lambda x: f"{x:.2%}")
    )

    # 5. Chuáº©n hÃ³a dá»¯ liá»‡u
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_balanced)
    X_test_scaled = scaler.transform(X_test)

    # 6. Huáº¥n luyá»‡n Logistic Regression (cÃ³ class_weight Ä‘á»ƒ tÄƒng nháº¡y cáº£m vá»›i lá»›p thiá»ƒu sá»‘)
    model = LogisticRegression(max_iter=5000, class_weight="balanced")
    model.fit(X_train_scaled, y_train_balanced)

    # 7. ÄÃ¡nh giÃ¡ trÃªn táº­p Test gá»‘c (giá»¯ nguyÃªn)
    y_pred = model.predict(X_test_scaled)
    y_prob = model.predict_proba(X_test_scaled)[:, 1]

    print("\nðŸŽ¯ Accuracy:", accuracy_score(y_test, y_pred))
    print("\nðŸ“‹ Classification Report:\n", classification_report(y_test, y_pred))

    # Biá»ƒu Ä‘á»“ Confusion Matrix trÆ°á»›c khi cÃ¢n báº±ng
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
    plt.title("Confusion Matrix (Test Set)")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

    # Biá»ƒu Ä‘á»“ Confusion Matrix cá»§a táº­p Train sau khi cÃ¢n báº±ng
    y_train_pred = model.predict(X_train_scaled)
    cm_train = confusion_matrix(y_train_balanced, y_train_pred)
    plt.figure(figsize=(5, 4))
    sns.heatmap(cm_train, annot=True, fmt="d", cmap="Greens", cbar=False)
    plt.title("Confusion Matrix (Train Set - Sau SMOTE)")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

    # ROC Curve & AUC
    fpr, tpr, thresholds = roc_curve(y_test, y_prob)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(6, 5))
    plt.plot(fpr, tpr, label=f"ROC Curve (AUC = {roc_auc:.2f})", lw=2)
    plt.plot([0, 1], [0, 1], "k--")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("Receiver Operating Characteristic (ROC) Curve")
    plt.legend(loc="lower right")
    plt.grid(True)
    plt.show()

    # 8. LÆ°u mÃ´ hÃ¬nh vÃ  scaler
    os.makedirs("models", exist_ok=True)
    joblib.dump(model, "models/trained_model.pkl")
    joblib.dump(scaler, "models/scaler.pkl")
    print("\nâœ… MÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i models/trained_model.pkl")
    print("âœ… Bá»™ chuáº©n hÃ³a Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i models/scaler.pkl")

    return model


def threshold_tuning_report(y_true, y_prob):
    thresholds = [i / 100 for i in range(10, 91, 5)]
    report = []

    for th in thresholds:
        y_pred = (y_prob >= th).astype(int)
        precision = precision_score(y_true, y_pred, zero_division=0)
        recall = recall_score(y_true, y_pred, zero_division=0)
        f1 = f1_score(y_true, y_pred, zero_division=0)
        acc = accuracy_score(y_true, y_pred)
        report.append({
            "Threshold": th,
            "Precision": round(precision, 2),
            "Recall": round(recall, 2),
            "F1-Score": round(f1, 2),
            "Accuracy": round(acc, 2)
        })

    df_report = pd.DataFrame(report)
    print("\nðŸ“Š Báº£ng so sÃ¡nh cÃ¡c threshold:")
    print(df_report)

    # Gá»£i Ã½ threshold tá»‘t nháº¥t theo F1
    best_row = df_report.loc[df_report["F1-Score"].idxmax()]
    print(f"\nðŸ’¡ Gá»£i Ã½ Threshold tá»‘t nháº¥t theo F1-score: {best_row['Threshold']} "
          f"(Precision: {best_row['Precision']}, Recall: {best_row['Recall']}, F1: {best_row['F1-Score']})")


    
